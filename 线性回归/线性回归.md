# 线性模型

## 回归问题

定义激活函数: $h_\theta=w^Tx+\theta_0 (w:(\theta_1,\theta_2,\theta_3...,\theta_n))$

由于为了评估预测值与实际输出值的差值，定义代价函数:$J(\theta_i)=\frac{1}{2n}\sum_{i =1}^{n}(h_\theta(x^{(i)})-y^{(i)})^2$

### 梯度下降

梯度下降是一个用来求函数最小值的算法，我们将使用梯度下降算法来求出代价函数$J(\theta_0,\theta_1)$的最小值。

梯度下降背后的思想是：开始时我们随机选择一个参数的组合$(\theta_0,\theta_1,......,\theta_n)$，计算代价函数，然后我们寻找下一个能让代价函数值下降最多的参数组合。我们持续这么做直到到到一个局部最小值（**local minimum**），因为我们并没有尝试完所有的参数组合，所以不能确定我们得到的局部最小值是否便是全局最小值（**global minimum**），选择不同的初始参数组合，可能会找到不同的局部最小值。

批量梯度下降算法的公式：$\theta_i:= \theta_i -\alpha \frac{\partial J(\theta_i)}{\partial \theta_i} $

其中是学习率$\alpha$（**learning rate**），它决定了我们沿着能让代价函数下降程度最大的方向向下迈出的步子有多大，在批量梯度下降中，我们每一次都同时让所有的参数减去学习速率乘以代价函数的导数。

### 梯度下降的线性回归

#### 普通的最小二乘法

梯度下降的更新函数：$\theta_j:= \theta_j -\alpha \frac{\partial J(\theta_i)}{\partial \theta_i} $

代价函数：$J(\theta_i)=\frac{1}{2n}\sum_{i =1}^{n}(h_\theta(x^{(i)})-y^{(i)})^2$

$\theta_0$:$\theta_0 = \theta_0 - \alpha\frac{1}{2m}\frac{\partial \sum_{i =1}^{m}(\theta_0+\theta_1x-y^{(i)})^2}{\partial\theta_0}=\theta_0 - \alpha\frac{1}{m}{ \sum_{i =1}^{m}(\theta_0+\theta_1x-y^{(i)})}=\theta_0 - \alpha\frac{1}{m}{ \sum_{i =1}^{m}(h_{\theta}(x^{i})-y^{(i)})}$

$\theta_{i>0}$:$\theta_i = \theta_i - \alpha\frac{1}{2m}\frac{\partial \sum_{i =1}^{m}(\theta_0+\theta_1x-y^{(i)})^2}{\partial\theta_1}=\theta_i - \alpha\frac{1}{m}{ \sum_{i =1}^{m}((\theta_0+\theta_1x-y^{(i)})x^{(i)})}=\theta_i - \alpha\frac{1}{m}{ \sum_{i =1}^{m}((h_{\theta}(x^{i})-y^{(i)})x^{(i)})}$

```python
from sklearn.linear_model import LinearRegression
model = LinearRegression()
```

#### 正规方程法

$\theta = (x^Tx)^{-1}x^TY$

由于小样本获得多特征比较困难，故使用正规方程法直接对矩阵求导，获得其解。

```python
import numpy as np
## x 为数据集的输入集，w为参数集，y为结果值
w = np.linalg.inv(x.T@x)@x.T@Y
```

### 广义线性模型

$y =g^{-1}(w^Tx+b)$

其中函数$g(·)$称为联系函数

### 过拟合

#### 岭回归(L2惩罚)

由于系数过大对于误差的影响较大，故通过二范数的范数增加惩罚力度，使得参数降低。

$J(\theta_j) = \frac{1}{2n}\sum_{i=1}^{n}(w^Tx+b-y_i)+\alpha||w_j||_2^2$ 其中j为要惩罚的参数下标

解为：$\beta = {(X^TX + \Gamma ^ T \Gamma)}^{-1}X \hat y$

```python
from sklearn.linear_model import Ridge
model = Ridge()
## Ridge()可以通过alpha来调节
```

#### Lasso回归

由于系数过大对于误差的影响较大，故通过一范数的范数增加惩罚力度，使得参数降低。

$J(\theta_j) = \frac{1}{2n}\sum_{i=1}^{n}(w^Tx+b-y_i)+\alpha||w_j||$其中j为要惩罚的参数的下标

```python
from sklearn.linear_model import Lasso
lasso = Lasso()
```

## 分类问题

### 逻辑回归

定义激活函数：$h_\theta = \frac{1}{1+e^{-\theta^Tx}}$

定义损失函数：$Cost(h_\theta,y) =\frac{1}{m}\sum_{i=1}^{m}[ -y^ilog(h_\theta(x^{(i)}))-(1-y^i)log(1-h_\theta(x^{(i)}))]$

求导更新参数为$\theta_i =\theta_i - \alpha\frac{1}{m}{ \sum_{i =1}^{m}((h_{\theta}(x^{i})-y^{(i)})x^{(i)})}$

### 多分类学习

#### OVM

将平面上所有种点n逐一标记为正类，其余为负类，经过n次后可以完成多分类任务。

这种多分类的处理方式，我们称之为One-Versus-All(OVA) Decomposition。这种方法的优点是简单高效，可以使用logistic regression模型来解决；缺点是如果数据类别很多时，那么每次二分类问题中，正类和负类的数量差别就很大，数据不平衡unbalanced，这样会影响分类效果。但是，OVA还是非常常用的一种多分类算法。

#### OVO

每次只取两类进行binary classification，取值为{-1，+1}，算得每个点是哪一类的概率，通过$ C_n^2 $此后，取概率的最高值，可以获得点的种类。

这种区别于OVA的多分类方法叫做One-Versus-One(OVO)。这种方法的优点是更加高效，因为虽然需要进行的分类次数增加了，但是每次只需要进行两个类别的比较，也就是说单次分类的数量减少了。而且一般不会出现数据unbalanced的情况。缺点是需要分类的次数多，时间复杂度和空间复杂度可能都比较高。

> class sklearn.linear_model.LogisticRegression(penalty='l2', *, dual=False, tol=0.0001, C=1.0, fit_intercept=True, intercept_scaling=1, class_weight=None, random_state=None, solver='lbfgs', max_iter=100, multi_class='auto', verbose=0, warm_start=False, n_jobs=None, l1_ratio=None
>
>
> - multi-class：分类方法是一对多还是一对一，选项有{‘ovr’,‘multinomial’}，前者是一对多。一对一更准，计算量也大
#### MVM

MvM将若干个作为正类，若干个作为负类。通过EOOC编码来预测。

编码： 对 N 个类别做M次划分， 每次划分将一部分类别划为正类， 一部分划为反类,从而形成一个二分类训练集； 这样一共产生M 个训练集,可训练出M 个分类器.

解码：M 个分类器分别对测试样本进行预测， 这些预测标记组成一个编码.将这个预测编码与每个类别各自的编码进行比较,返回其中距离最小的类别作为最终预测结果。

### 类别不平衡问题

再放缩：$\frac{y}{1-y}>\frac{m^+}{m^-}$用来解决类别不平衡，正类与负类相差特别的情况