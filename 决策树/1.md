# 决策树(Decision Tree)

只有一棵决策树树：$G(x) = \sum_{t=1}^{T}q_t(x)g_t(x)$   $g_t$是结果，$q_t$是$g_t$成立的条件

决策树下有子树：$G(X) = \sum_{c=1}^{C}[b(x)=c]G_c(x)$

$b(x)$:是$G_c(x)$成立的条件，$G_c(x)$：子树

训练流程：首先学习设定划分不同分支的标准和条件是什么；接着将整体数据集D根据分支个数C和条件，划为不同分支下的子集$D_c$;然后对每个分支下的$D_c$进行训练，得到相应的机器学习模型$G_c$;最后将所有分支下的$G_c$合并到一起成大矩$G(x)$。但这种递归需要终止条件，满足条件后，返回基本的$g_t(x)$。

决策树的基本演算包含了四个条件：

* 分支条件
* 分支个数
* 终止条件
* 基本算法

## C&RT
### 基本原理
C&RT基本假设：C=2(二叉树)；最后每个分支的$g_t(x)$是一个常数。

对于分类问题，比较正类和负类哪个更多，$g_t(x)$取所占比例最多的那一类$y_n$;对于回归问题，$g_t(x)$取所有$y_n$的平均值。

损失函数:$b(x) = argmin_{decision\ stumps\ h(x)} \sum_{c=1}^2|D_c\  with\  h|·impurity(D_c\ with\ h)$
### 损失函数
purifying的核心思想就是每次切割都尽可能让左子树和右子树中同类样本占得比例最大或者$y_n$都很接近（regression），即错误率最小。比如说classifiacation问题中，如果左子树全是正样本，右子树全是负样本，那么它的纯净度就很大，说明该分支效果很好。

根据C&RT中purifying的思想，我们得到选择合适的分支条件b(x)的表达式如上所示。最好的decision stump重点包含两个方面：一个是刚刚介绍的分支纯净度purifying，purifying越大越好，而这里使用purifying相反的概念impurity，则impurity越小越好；另外一个是左右分支纯净度所占的权重，权重大小由该分支的数据量决定，分支包含的样本个数越多，则所占权重越大，分支包含的样本个数越少，则所占权重越小。上式中的$|D_c\ with\ h|$代表了分支c所占的权重。这里b(x)类似于error function（这也是为什么使用impurity代替purifying的原因），选择最好的decision stump，让所有分支的不纯度最小化，使b(x)越小越好。

impurity

对于回归问题： $impurity(D) = \frac{1}{N}\sum_{n=1}^{N}(y_n - \hat y)^2$， $\hat y$表示对应分支下所有$y_n$的均值

对于分类问题： $impurtity(D) = \frac{1}{N}\sum_{n=1}^{N}[y_n≠y^*]$  其中$y^*$表示对应分支下所占比例最大的那一类

分类损失函数：$1-\sum_{k = 1}^K(\frac{\sum_{n=1}^{N}[y_n =K]}{N})^2$ K为分支个数

C&RT算法迭代终止条件有两种情况，第一种情况是当前各个分支下包含的所有样本$y_n$都是同类的，即不纯度impurity为0，表示该分支已经达到了最佳分类程度。第二种情况是该特征下所有的$x_n$相同，无法对其进行区分，表示没有decision stumps。遇到这两种情况，C&RT算法就会停止迭代。
### 正则化

防止模型过于复杂的方法是减少叶子$(g_t(x))$的数量，那么令regularizer就为决策树中叶子的总书，记为$\Omega(G)$。

正则化的目的是尽可能减少$\Omega(G)$的值。这样，正则决策树可以表达成：$argmin_{all\ possible\ G}E_{in}(G)+\lambda\Omega(G)$

那么如何确定修剪多少叶子，修剪哪些叶子呢？假设由C&RT算法得到一棵完全长成树（fully-grown tree），总共10片叶子。首先分别减去其中一片叶子，剩下9片，将这10种情况比较，取$E_{in}$最小的那个模型；然后再从9片叶子的模型中分别减去一片，剩下8片，将这9种情况比较，取$E_In$最小的那个模型。以此类推，继续修建叶子。这样，最终得到包含不同叶子的几种模型，将这几个使用regularized decision tree的error function来进行选择，确定包含几片叶子的模型误差最小，就选择该模型。另外，参数$\lambda$可以通过validation来确定最佳值。

## 随机森林

增加多样性，防止过拟合：

1.随机选择样本

2.将现有的特征x，通过数组p进行线性组合来保持多样性 $\phi_i(x)=p_i^Tx$

不同分支i下的$p_i$是不同的，而且向量$p_i$中大部分元素为零，因为我们选择的只是一部分特征，这是一种低维映射。